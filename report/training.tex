The aim of this section is to create the most accurate tree for each label. To do so we need to go through the following steps : 

Binary-target creation : 
See folder data\_loader.py
In order to create a tree for a given label the probleme need to be reduced in a binary one. Then the creation of binary target array for each label enable to classify examples into "success" and "fail" according to it. This step is essential to be able to manage the best attribute selection and the 10-fold-cross-validation afterward.


Selecting the best attribute for each node : 
See folder training.py
The selection of the best attribute allows to create the most efficient tree for each label, this ensure to have one of the shortest tree and an optimal search. For a given attribute, we can distinguish positives and negatives example according to the value (respectively 1 and 0) of the attribute position in the example. Then the binary target classify these positives and negatives examples into success(p) and fail(n). 

More precisely, for each attribute we can calculate it's related gain at each step of the tree creation (node). Let p0 and (respectively n0) be the success(fail) of the positives examples and p1, n1 for the negative one. The evaluation of the gain is done according to the following relations : 

I(p,n) is the corresponding entropy for the whole set of examples.The highest is the gain the best will be the attribute to divide label examples. As the Entropy in constant at a given  we decided to minimize the reminder of each attribute in order to do the best choice. 
