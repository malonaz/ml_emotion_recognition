
1) Data loader module\\
\emph{load\_data} extracts the data from one of the two datasets
such as \emph{cleandata\_student.mat} into two matrices to separate the examples and labels. \\
\emph{get\_binary\_targets} maps a matrix of labels to a binary target for a given label
such that a '1' indicates a positive label and a '0' indicates a negative label.

2) Training module \\
Let us start by discussing our \emph{find\_best\_attribute} implementation. \\
First, when seeking the best attribute to split the current node's examples on, we do not maximize the information gain.
Instead, we minimize the information remainder (i.e. the average entropy of the two children nodes
that would be created if splitting on a particular attribute).
This method yields the same results yet
improves the algorithm's complexity, as we are not repetitively recomputing the initial entropy of the node.\\
Second, we noticed that deeper down the tree, multiple attributes may be candidates to be the best attribute on a certain node,
because they offer the same information gain. We were faced with a dilemma: Should we simply
take the first one of those candidate attributes? We decided to randomize a selection from the candidates.\\\\


2) cross validation perform
3) how you compute average results
4) how to test python code

