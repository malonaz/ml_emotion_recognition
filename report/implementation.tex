
Part 1) Loading the data\\
We have created a data loader module which serves two purposes.
First, its \emph{load\_data} function extracts the data from one of the two datasets
and returns two matrices containing the examples and labels. \\
Second, its \emph{get\_binary\_targets} function maps a matrix of labels to a binary target for a chosen emotion,
such that a '1' indicates that a label represents this emotion and a '0' indicates otherwise.

2) Training module \\
Let us start now turn to our \emph{choose\_best\_decision\_attribute} implementation. \\
First, when seeking the best attribute to split the current node's examples on, we do not maximize the information gain.
Instead, we minimize the information remainder (i.e. the average entropy of the two children nodes
that would be created if splitting on a particular attribute).
This method yields the same results yet
improves the algorithm's complexity, as we are not repetitively recomputing the initial entropy of the node.\\
Second, we noticed that deeper down the tree, multiple attributes may be candidates to be the best attribute on a certain node,
because they offer the same information gain. We were faced with a dilemma: Should we simply
take the first one of those candidate attributes? We decided to randomize a selection from the candidates.

Next, let us take a closer look at the implementation \emph{find\_majority\_value}.\\
What value should it return when the given binary targets contains an equal number of
'1's and '0's? We decided to randomize the selection.

Now, let us discuss how we performed cross-validation\\
Here is the general process we follow:
split the data randomly into 10 folds.




2) cross validation perform



3) how you compute average results
4) how to test python code

