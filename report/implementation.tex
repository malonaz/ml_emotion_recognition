We have divided the program into four modules. Let us walk through each module's functionality \& purpose.

Module 1: Loading the Data \\
This module serves two purposes.\\
First, its \emph{load\_data()} function extracts the data from one of the two datasets
and returns two matrices containing the examples and labels, respectively. \\
Second, its \emph{get\_binary\_targets()} function maps a matrix of labels to a matrix of binary targets for a chosen emotion,
such that a '1' indicates that a label represents this emotion and a '0' indicates otherwise.

Module 2: Decision Tree Structure \\
This module contains the structure of a decision tree, which is made up of two types of objects. \\
The \emph{Node} class represents an internal (non-leaf) node in the decision tree and is completely characterized by the attribute
this node is testing as well as a kids array containing any children of this node. In this coursework, there are two children
in an internal node, one for examples that test negative for this node's attribute and one for examples that test positive.
In addition, each \emph{node} has an id member used for graph generation purposes.
A Node has an evaluate method which, given an example, check this example's value for the attribute it is responsible for testing.
It uses this information to recurse into the appropriate kid's evaluate method. \\
The \emph{LeafNode} class represents a leaf node in the decision tree and is a subclass of the \emph{Node} class.
Thus, it inherits all its field, in addition to a new member representing the class label of this leaf node,
a '0' or a '1' representing the decision of the tree.
Lastly, it overrides the evaluate method of the Decision tree as there is no recursion at a leaf node, it simply returns its class label.\\


Module 3: Training Decision Trees \\
Let us begin this module's discussion with the implementation of \emph{choose\_best\_decision\_attribute()}. \\
First, when seeking the best attribute to split the current node's examples on, we do not maximize the information gain.
Instead, we minimize the information remainder (i.e. the average entropy of the two children nodes
that would be created if splitting on a particular attribute).
This method yields the same results yet improves the algorithm's complexity,
as we are not repetitively recomputing the initial entropy of the node.
Second, we noticed that deeper down a tree, multiple attributes may be candidates to be the best attribute on a certain node,
because they offer the same information gain. We were faced with a dilemma: Should we simply
take the first one of those candidate attributes? We decided to randomize a selection from the candidates.\\
Next, let us take a closer look at the implementation \emph{find\_majority\_value()}.\\
What value should it return when the given binary targets contains an equal number of
'0's and '1's? We decided to randomize the selection.\\
We have created a \emph{train\_trees()} function that abstracts aways the lower-level workings of the training
of trees. This function takes two matrices, examples and labels respectively,
and returns six trained trees, one for each emotion.
Lastly, we have a function \emph{vizualize\_trees()} that takes a list of trees and generates a graph dot file
for each tree.

Module 4: Evaluation \\
The \emph{get\_k\_folds()} function splits the given data into k folds and returns the k folds in a list.
The function generates a random a random integer from the range of indices
of the remaining examples, pops it into the current fold, until the current fold reaches the fold size implied
by the k way splitting of the examples. Here we have 1004 examples and as such, create 10 folds of 100 examples.
We choose not to include the extra 4 in any of the folds in order to keep the folds equal in magnitude.\\
\emph{test\_trees()} takes trained trees and a list of examples to return a vector label predictions.\\
\emph{get\_error\_rate()} returns the error rates of predictions versus their true values, as dictated by the specs.\\
\emph{get\_get\_k\_folds()} creates k folds from the given examples and labels. the k folds obtained are always the same
because we seed the random number generator, for testing purposes. This way, we can compare different methods of breaking ties
when classifying examples with the \emph{test\_trees()} function.
Here we use 10 folds, which does not divide well into 1004 or 1001, the respective sizes of the clean and noisy datasets.
To make sure we use the max number of examples, we get 9 folds randomly, and assign the last 104 or 101 examples to the
10th fold.\\
Finally, let's talk about the \emph{cross\_validation()}.
For each fold, we train six trees on the rest of the folds. Next, we compute the vector of predictions on the current fold's data.
The prediction vector of each fold and its corresponding true label vector are concatenated and used to generate the average confusion
matrix and the average error rate.
