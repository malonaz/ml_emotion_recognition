While attempting to classify examples using six trained trees, we came to face two ambiguous situations.\\
The first occurs when an example does not match any emotion.
Here, with little information to play with, we randomly choose an emotion.\\
The second ambiguous situation arises when an example is claimed by multiple emotions.
We explored three methods to deal with this scenario.
The first is trivial and simply chooses one of the candidate emotions at random.
For the other solutions, we decided to use more information to make a decision and so we altered our classification algorithm
to return not only the decision ('1' or '0' for yes or no), but also the depth within a tree at which this solution was found.
Perhaps there is more confidence in a decision made on two attributes rather than 10? Perhaps it is the other way around.
We decided to experiment and find out for ourselves. Recall that in our implementation section, we mentioned that before getting
the k folds, we seed the random number generator, such that folds from the same data sets are always the same. We introduced this
in order to be able to compare these three methods. (no comparisons are drawn across different datasets here). \\

\include*{report/average_classification_rate_clean}

\include*{report/average_classification_rate_noisy}


We ran the algorithms 100 times, with different seed values, to get an average of the performances of these three methods.
The table above displays the results of our experiment.
For both datasets, it seems that giving preference to nodes at shallow levels provides a higher classification rate.
More telling though, is the random strategy outperforming the deepest node strategy.
Intuitively, giving preference to deep nodes exposes the model to overfitting than random or shallow strategies would, as deeper nodes
deal with fewer number of examples and will choose an attribute test tailored to small subsets of the datasets.
We chose to implement the shortest method as default to classify emotions.

