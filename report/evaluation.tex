In this section, we will attempt to evaluate our decision tree model. We begin by using a 10-fold cross-validation
in order to assess the precision and accuracy of our model. 

Let's take a closer look at the average confusion matrices of the datasets.
The relatively high numbers on the diagonal indicate that each emotion's decision tree correctly
identifies an example more often than not. Such observations are more easily infered from metrics such as the precision or recall rate.
The interest in the confusion matrix lies in examining where the misclassified examples get classified or to be more specific,
which emotion misappropriates it. 

\newpage
\include*{report/confusion_matrix_clean}

The above confusion matrix reveals interesting aspects of the clean dataset.
The Anger decision tree misclassifies over 10\% of Sadness examples as Anger, but only 1\% of Surprise examples.
Similarly, the Sadness decision tree misappropriates 10\% of Disgust examples.
Such patterns emerge upon closer examination of these metrics: Fear is often confused with Surprise, but Happiness is rarely confused with Fear.
Intuitively, since we are looking at a clean dataset,
the reason behind these misclassifications may take root in the muscle activations being similar for some emotions, such as Sadness and Disgust.
This could be improved on by including  more than 45 facial muscles.
Another reason for these confusions could arise from the data being collected from mixed emotions or perhaps because facial display
of emotion is inherently subjective in nature.

\include*{report/recall_and_precision_rates_clean}

The table above shows the precision and recall rates for the clean dataset, as well as the F\textsubscript{1} measure,
where we weight the two aforementioned rates equally. These rates are computed from the confusion matrix and allow us to
observe the bigger picture. Here, we can see that Happiness and Surprise are the most precisely and accurately recognizable
emotions, while Sadness is the least. \\



We now wish to get the classification error of the model, the model being the combination of our six trees.
Hence, we choose to treat the model as a blackbox and wish to evaluate the rate at which our model classifies emotions correctly.
We first compute the average error rate using the following formula.
\include*{report/equations}
We then compute our classification rate as (1 - the average error rate).
For the clean dataset, the classification clocks in at 71.6\%.
Let us now turn to the noisy dataset, in hopes of seeing more questions answered.

 
\include*{report/confusion_matrix_noisy}
\newpage
Confusion matrices make for poor comparison tools when it comes to comparing two datasets.
Beyond the fact that the two datasets don't have the same number of data points,
they also do not come in the same proportions for each emotion. For example, there are 132 Anger examples in the clean dataset but only
88 in the noisy dataset. As such, we abstain from making absolute comparisons and instead discuss patterns within the rows
of this confusion matrix. We observe again that Anger claims many examples of Sadness but also Fear this time. Perhaps one or two facial
muscles which are responsible for distinguishing Fear from Sadness, are obfuscated by the noise.
It seems that the noise has exacerbated patterns exhibited in the clean dataset.

\include*{report/recall_and_precision_rates_noisy}

The F\textsubscript{1} measure rates for Anger and Sadness tumble to a paltry $\sim$28\% and $\sim$38\% respectively.
It seems the few attributes that allowed some identification of these emotions do not survive the noise.
Happiness and Surprise remain well identified, albeit at a lower percentage.
We observe an average classification rate of 60.6\% for the noisy dataset.


We can see that trees trained on the clean, or noisy dataset, share similar patterns.
Happiness exhibits the highest rate of recognition and Sadness the least.
This general pattern is perhaps due to the fact that we have plenty of data for Happiness ($\sim$210) versus Sadness ($\sim$120).



