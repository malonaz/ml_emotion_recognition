In this section, we will attempt to evaluate our decision tree model. We begin by using a 10-fold cross-validation
in order to assess the precision and accuracy of our model. 

To begin, let's take a closer look at the average confusion matrices of the datasets.
The relatively high numbers on the diagonal indicate that each emotion's decision tree correctly
identifies an example more often than not. Such observations are more easily infered from metrics such as the precision or recall rate.
The interest in the confusion matrix lies in examining where the misclassified examples end or to be more specific,
which emotion misappropriates it. 

\include*{confusion_matrix_clean}

The above confusion matrix reveals interesting aspects of the clean dataset.
The Anger decision trees misclassifies over 10\% of Sadness examples as Anger, but only 1\% of Surprise examples.
Similarly, the Sadness decision tree misappropriates 10\% of Disgust examples.
Such patterns emerge upon closer examination of such metrics: Fear is often confused with Surprise, but Happiness is rarely confused with Fear.
Intuitively, since we are looking at a clean dataset,
the reason behind these misclassifications may take root in the muscle activations being similar for some emotions, such as Sadness and Disgust.
This could be improved on by including  more than 45 facial muscles.
Another reason for these confusions could arise from the data being collected from mixed emotions, perhaps because facial display
of emotion is inherently subjective in nature.

\include*{recall_and_precision_rates_clean}

The table above shows the precision and recall rates for the clean dataset, as well as the F\textsubscript{1} measure,
where we weight the two aforementioned rates equally. These rates are computed from the confusion matrix and allow us to
observe the bigger picture. Here, we can see that Happiness and Surprise are the most precisely and accurately recognizable
emotions, while Sadness is the least. The average classification rate computed from the cross validation's average error rate
clocks in a 71.6\%. Let us now turn to the noisy dataset, in hopes of seeing questions answered.

 
\include*{confusion_matrix_noisy}

The main difference for the noisy data is that the diagonal of the matrix show less matches than the clean data set, but stay the main recognition for the emotions.
We can still observe some confusions between labels, especialluy between disgust sadness and anger, and the tendances that we state for clean data set remain the same (recognition of surprise and happinness).


\include*{recall_and_precision_rates_noisy}

The noisy data set confirm the results made on clean data set, and clearly show an higher recognition for happiness and suprised. However, even if sadness still shows a low F1 measure, anger have a lower measure than all the others. This can be explained by the fact that anger and sadness can be confused in the real world, which is closer to noisy data. 


average classification rate:
60.6\%


CONCLUSION OF EVALUATION

We can see that when using both strategies to determine the class, Class 4 (happiness) seems to have higher recognition, and Class 5 (sadness) is what gets recognised least and is mostly confused with anger. This may also be due to the fact that we have plenty of data for class 4 and not so much for class 5.
From the performance analysis of noisy data, we can see that again class 4 is recognised with high precision when using balanced tree classification and class 2 (disgust) is recognised with more precision when using length based classification. Both strategies have difficulties in identifying class 1 and class 5.

TITLE: CLASSIFICATION RATE

Finally the classification rate show that with clean data we can classify more emotions than with noisy data, which could seam logical. 
We can also state the tree are well trained beacuse we have more than 60\% for each data set wich are good rates. 




